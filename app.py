import os
import fitz  # PyMuPDF
import gradio as gr
import requests
from sentence_transformers import SentenceTransformer
import faiss
from dotenv import load_dotenv

# Load API key from .env
load_dotenv()
api_key = os.getenv("OPENROUTER_API_KEY")

# Check if key loaded
if not api_key:
    raise ValueError("‚ùå OPENROUTER_API_KEY not found in .env file!")

# Load embedding model
embed_model = SentenceTransformer("all-MiniLM-L6-v2")
dimension = 384
index = faiss.IndexFlatL2(dimension)
stored_chunks = []

# --- Chunking PDF ---
def chunk_pdf_text(file_path, chunk_size=500):
    doc = fitz.open(file_path)
    text = ""
    for page in doc:
        text += page.get_text()
    words = text.split()
    return [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]

# --- Store Embeddings ---
def store_chunks(chunks):
    embeddings = embed_model.encode(chunks)
    index.add(embeddings)
    stored_chunks.extend(chunks)

# --- Search ---
def retrieve_relevant_chunks(query, k=5):
    query_embed = embed_model.encode([query])
    D, I = index.search(query_embed, k)
    return [stored_chunks[i] for i in I[0]]

# --- Call OpenRouter LLM ---
def generate_answer(query, context):
    prompt = f"""Use the following context to answer the question.
Context:
{context}

Question: {query}
Answer:"""

    headers = {
        "Authorization": f"Bearer {api_key}",
        "HTTP-Referer": "https://openrouter.ai",  # optional but useful
        "X-Title": "academic-rag-assistant"
    }

    payload = {
        "model": "mistralai/mistral-7b-instruct",  # you can change model here
        "messages": [
            {"role": "system", "content": "You are a helpful academic assistant."},
            {"role": "user", "content": prompt}
        ]
    }

    response = requests.post("https://openrouter.ai/api/v1/chat/completions",
                             headers=headers, json=payload)

    try:
        return response.json()['choices'][0]['message']['content']
    except Exception as e:
        return f"Error: {str(response.text)}"

# --- Main Gradio Function ---
def process_file_and_query(pdf_file, query):
    try:
        chunks = chunk_pdf_text(pdf_file.name)
        store_chunks(chunks)
        relevant_chunks = retrieve_relevant_chunks(query)
        context = " ".join(relevant_chunks)
        return generate_answer(query, context)
    except Exception as e:
        return f"‚ùå Error: {str(e)}"

# --- Gradio UI ---
gr.Interface(
    fn=process_file_and_query,
    inputs=[
        gr.File(label="üìÑ Upload PDF", type="filepath"),
        gr.Textbox(label="üí¨ Ask a question")
    ],
    outputs="text",
    title="üìö Academic RAG Assistant (OpenRouter)",
    description="Upload a research paper PDF and ask a question based only on its content. Powered by OpenRouter LLM + RAG."
).launch(share=True)
